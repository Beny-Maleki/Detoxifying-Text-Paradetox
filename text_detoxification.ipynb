{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h3 style=\"color:red\"> Warning: This document might contain harmful language </h3>"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T12:34:35.809045Z","iopub.status.busy":"2024-02-04T12:34:35.808679Z","iopub.status.idle":"2024-02-04T12:34:50.950211Z","shell.execute_reply":"2024-02-04T12:34:50.949214Z","shell.execute_reply.started":"2024-02-04T12:34:35.809006Z"},"trusted":true},"outputs":[],"source":["### Project was ran in a kaggle notebook\n","# install packages\n","!pip install wget -q\n","!pip install transformers torch -q"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-04T12:34:50.952545Z","iopub.status.busy":"2024-02-04T12:34:50.952209Z","iopub.status.idle":"2024-02-04T12:34:58.311626Z","shell.execute_reply":"2024-02-04T12:34:58.310855Z","shell.execute_reply.started":"2024-02-04T12:34:50.952516Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import os\n","import wget\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim import AdamW\n","from sklearn.model_selection import train_test_split\n","# Importing the T5 modules from huggingface/transformers\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","# WandB – Import the wandb library\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T12:34:58.313129Z","iopub.status.busy":"2024-02-04T12:34:58.312712Z","iopub.status.idle":"2024-02-04T12:34:58.340828Z","shell.execute_reply":"2024-02-04T12:34:58.339814Z","shell.execute_reply.started":"2024-02-04T12:34:58.313105Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"markdown","metadata":{},"source":["<h2> Loading the Dataset </h2>\n","<p> Initially, we download the data and load it in a dataframe.\n","</p>"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T12:34:58.342559Z","iopub.status.busy":"2024-02-04T12:34:58.342170Z","iopub.status.idle":"2024-02-04T12:34:58.657630Z","shell.execute_reply":"2024-02-04T12:34:58.656715Z","shell.execute_reply.started":"2024-02-04T12:34:58.342495Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>toxic</th>\n","      <th>neutral1</th>\n","      <th>neutral2</th>\n","      <th>neutral3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>he had steel balls too !</td>\n","      <td>he was brave too!</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>dude should have been taken to api , he would ...</td>\n","      <td>It would have been good if he went to api. He ...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>im not gonna sell the fucking picture , i just...</td>\n","      <td>I'm not gonna sell the picture, i just want to...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>the garbage that is being created by cnn and o...</td>\n","      <td>the news that is being created by cnn and othe...</td>\n","      <td>The news that is being created by cnn and othe...</td>\n","      <td>the garbage that is being created by cnn and o...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>the reason they dont exist is because neither ...</td>\n","      <td>The reason they don't exist is because neither...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11922</th>\n","      <td>this is the \" dumb \" shit they 're laughing at</td>\n","      <td>this is the \" nonsense \" situation they 're la...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11923</th>\n","      <td>no seriously you 're fucking retarded</td>\n","      <td>no seriously you 're slow..</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11924</th>\n","      <td>christians love to shit on the pope .</td>\n","      <td>christians love to criticize the pope</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11925</th>\n","      <td>but if saying \" fuck that group \" is much more...</td>\n","      <td>but if saying\" that group is bad\" is much more...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11926</th>\n","      <td>shit we probably literally blow that up in a w...</td>\n","      <td>We probably litteralky blow that up in a week.</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11927 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                   toxic  \\\n","0                               he had steel balls too !   \n","1      dude should have been taken to api , he would ...   \n","2      im not gonna sell the fucking picture , i just...   \n","3      the garbage that is being created by cnn and o...   \n","4      the reason they dont exist is because neither ...   \n","...                                                  ...   \n","11922     this is the \" dumb \" shit they 're laughing at   \n","11923              no seriously you 're fucking retarded   \n","11924              christians love to shit on the pope .   \n","11925  but if saying \" fuck that group \" is much more...   \n","11926  shit we probably literally blow that up in a w...   \n","\n","                                                neutral1  \\\n","0                                      he was brave too!   \n","1      It would have been good if he went to api. He ...   \n","2      I'm not gonna sell the picture, i just want to...   \n","3      the news that is being created by cnn and othe...   \n","4      The reason they don't exist is because neither...   \n","...                                                  ...   \n","11922  this is the \" nonsense \" situation they 're la...   \n","11923                        no seriously you 're slow..   \n","11924              christians love to criticize the pope   \n","11925  but if saying\" that group is bad\" is much more...   \n","11926     We probably litteralky blow that up in a week.   \n","\n","                                                neutral2  \\\n","0                                                    NaN   \n","1                                                    NaN   \n","2                                                    NaN   \n","3      The news that is being created by cnn and othe...   \n","4                                                    NaN   \n","...                                                  ...   \n","11922                                                NaN   \n","11923                                                NaN   \n","11924                                                NaN   \n","11925                                                NaN   \n","11926                                                NaN   \n","\n","                                                neutral3  \n","0                                                    NaN  \n","1                                                    NaN  \n","2                                                    NaN  \n","3      the garbage that is being created by cnn and o...  \n","4                                                    NaN  \n","...                                                  ...  \n","11922                                                NaN  \n","11923                                                NaN  \n","11924                                                NaN  \n","11925                                                NaN  \n","11926                                                NaN  \n","\n","[11927 rows x 4 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["if not os.path.exists('./paradetox.tsv'):\n","    url_csv = 'https://raw.githubusercontent.com/s-nlp/paradetox/main/paradetox/paradetox.tsv'\n","    wget.download(url_csv, 'paradetox.tsv')\n","    \n","df = pd.read_csv(\"paradetox.tsv\", sep='\\t')\n","\n","df"]},{"cell_type":"markdown","metadata":{},"source":["<p> Now, we define the Dataset class. Here, we are flattening each row of toxic text to at most three different neutral rows.</p>\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T12:35:29.472918Z","iopub.status.busy":"2024-02-04T12:35:29.472165Z","iopub.status.idle":"2024-02-04T12:35:29.482689Z","shell.execute_reply":"2024-02-04T12:35:29.481720Z","shell.execute_reply.started":"2024-02-04T12:35:29.472884Z"},"trusted":true},"outputs":[],"source":["class ParaDetoxDataset(Dataset):\n","    def __init__(self, tokenizer, data, max_length=512):\n","        self.tokenizer = tokenizer\n","        # Expand the dataset to include a row for each neutral alternative\n","        self.data = []\n","        self.max_length = max_length\n","        for _, row in data.iterrows():\n","            toxic_text = row['toxic']\n","            for neutral in ['neutral1', 'neutral2', 'neutral3']:\n","                if pd.notnull(row[neutral]):  # Ensure the neutral text is not null\n","                    self.data.append((toxic_text, row[neutral]))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        toxic_text, neutral_text = self.data[index]\n","        \n","        inputs = self.tokenizer.encode_plus(\n","            \"Detoxify following sentence from bad words: \" + toxic_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            return_token_type_ids=False,\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        targets = self.tokenizer.encode_plus(\n","            neutral_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            return_token_type_ids=False,\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'labels': targets['input_ids'].flatten()\n","        }"]},{"cell_type":"markdown","metadata":{},"source":["<p> Before splitting the dataset and wrapping the train and test datasets with the defined Dataset class, we initialize the text tokenizer with the pretrained tokenizer of T, the model that we are going to use as our base model. </p>\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T12:35:31.527647Z","iopub.status.busy":"2024-02-04T12:35:31.526818Z","iopub.status.idle":"2024-02-04T12:35:34.256540Z","shell.execute_reply":"2024-02-04T12:35:34.255681Z","shell.execute_reply.started":"2024-02-04T12:35:31.527611Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2f8b17770f14f789464e6f6eb890d3d","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24744916619b4c2ebbceb560cef90827","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f57c3c7d26b04206990bc35c9b5d57ee","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# Initialize tokenizer\n","model_name = 't5-small' # You can choose other versions based on your needs and computational resources\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","\n","# Prepare dataset\n","df_train, df_val = train_test_split(df, test_size=0.1)\n","train_dataset = ParaDetoxDataset(tokenizer, df_train)\n","val_dataset = ParaDetoxDataset(tokenizer, df_val)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8)\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2> Training Phase (Fine-Tune) </h2>"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T12:35:43.612946Z","iopub.status.busy":"2024-02-04T12:35:43.612264Z","iopub.status.idle":"2024-02-04T12:35:44.290097Z","shell.execute_reply":"2024-02-04T12:35:44.289150Z","shell.execute_reply.started":"2024-02-04T12:35:43.612913Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["model = T5ForConditionalGeneration.from_pretrained(model_name)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","print(device)\n","\n","optimizer = AdamW(model.parameters(), lr=3e-4)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T13:07:22.537100Z","iopub.status.busy":"2024-02-04T13:07:22.536742Z","iopub.status.idle":"2024-02-04T13:07:22.641476Z","shell.execute_reply":"2024-02-04T13:07:22.640591Z","shell.execute_reply.started":"2024-02-04T13:07:22.537072Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'This situation should have got better.'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["\n","def generate_sentence(sentence):\n","    \n","    input_ids = tokenizer.encode(\"Detoxify following sentence from bad words: \" + sentence, return_tensors=\"pt\").to(device)\n","    output = model.generate(input_ids = input_ids,\n","                            max_length=200, \n","                            num_beams=2,\n","                            repetition_penalty=2.5, \n","                            length_penalty=1.0, \n","                            early_stopping=True)\n","    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return generated_sentence\n","\n","sentence = \"Fuck this situation, This shitty loss should have got better.\"\n","\n","generate_sentence(sentence)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T13:08:01.762698Z","iopub.status.busy":"2024-02-04T13:08:01.761846Z","iopub.status.idle":"2024-02-04T13:22:03.376238Z","shell.execute_reply":"2024-02-04T13:22:03.375381Z","shell.execute_reply.started":"2024-02-04T13:08:01.762661Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2221/2221 [14:01<00:00,  2.64it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.02283197072284688\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["epochs = 3\n","from tqdm import tqdm\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(train_loader):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        \n","        outputs = model(input_ids=input_ids, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","    \n","    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T13:23:32.462060Z","iopub.status.busy":"2024-02-04T13:23:32.461683Z","iopub.status.idle":"2024-02-04T13:23:33.052755Z","shell.execute_reply":"2024-02-04T13:23:33.051903Z","shell.execute_reply.started":"2024-02-04T13:23:32.462026Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["*********************\n","Fuck this situation, This shitty loss should have got better.\n","This situation should have got better.\n","*********************\n","Oh for fuck sake. This crazy moron is not even getting better.\n","Oh for sake sake. This person is not even getting better.\n","*********************\n","Come on move your lazy ass!\n","Come on move.\n","*********************\n","no seriously you 're fucking retarded\n","No seriously you're retarded\n","*********************\n","he had steel balls too\n","he had steel balls too\n","*********************\n","christians love to shit on the pope .\n","christians love the pope.\n","*********************\n"]}],"source":["# Examples taken from the dataset\n","instance1 = \"Fuck this situation, This shitty loss should have got better.\"\n","instance2 = \"Oh for fuck sake. This crazy moron is not even getting better.\"\n","instance3 = \"Come on move your lazy ass!\"\n","print('*********************')\n","print(instance1)\n","print(generate_sentence(instance1))\n","print('*********************')\n","print(instance2)\n","print(generate_sentence(instance2))\n","print('*********************')\n","print(instance3)\n","print(generate_sentence(instance3))\n","print('*********************')\n","print('no seriously you \\'re fucking retarded')\n","print(generate_sentence('no seriously you \\'re fucking retarded'))\n","print('*********************')\n","print('he had steel balls too')\n","print(generate_sentence('he had steel balls too'))\n","print('*********************')\n","print('christians love to shit on the pope .')\n","print(generate_sentence('christians love to shit on the pope .'))\n","print('*********************')\n"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T13:31:24.957585Z","iopub.status.busy":"2024-02-04T13:31:24.956740Z","iopub.status.idle":"2024-02-04T13:31:25.098106Z","shell.execute_reply":"2024-02-04T13:31:25.097172Z","shell.execute_reply.started":"2024-02-04T13:31:24.957551Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["com see everyone flipping shit in that thread ?\n","com see everyone flipping in that thread ?\n","com see everyone flipping things in that thread?\n"]}],"source":["ind = 8091\n","print(df.iloc[ind]['toxic'])\n","print(df.iloc[ind]['neutral1'])\n","print(generate_sentence(df.iloc[ind]['toxic']))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from huggingface_hub import notebook_login\n","\n","# notebook_login()\n","# tokenizer.push_to_hub('detoxify')\n","# model.push_to_hub('detoxify')\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T13:23:46.583085Z","iopub.status.busy":"2024-02-04T13:23:46.582256Z","iopub.status.idle":"2024-02-04T13:26:58.907806Z","shell.execute_reply":"2024-02-04T13:26:58.906879Z","shell.execute_reply.started":"2024-02-04T13:23:46.583048Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 248/248 [03:12<00:00,  1.29it/s]\n"]}],"source":["model.eval()\n","predictions = []\n","actuals = []\n","with torch.no_grad():\n","    for data in tqdm(val_loader):\n","        y = data['labels'].to(device, dtype = torch.long)\n","        ids = data['input_ids'].to(device, dtype = torch.long)\n","        mask = data['attention_mask'].to(device, dtype = torch.long)\n","\n","        generated_ids = model.generate(\n","            input_ids = ids,\n","            attention_mask = mask, \n","            max_length=150, \n","            num_beams=2,\n","            repetition_penalty=2.5, \n","            length_penalty=1.0, \n","            early_stopping=True\n","        )\n","        preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","        target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","        \n","        predictions.extend(preds)\n","        actuals.extend(target)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T13:27:19.312264Z","iopub.status.busy":"2024-02-04T13:27:19.311905Z","iopub.status.idle":"2024-02-04T13:27:19.317942Z","shell.execute_reply":"2024-02-04T13:27:19.316979Z","shell.execute_reply.started":"2024-02-04T13:27:19.312233Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Actual: sit down\n","Predicted: sit down lol\n","*******************************\n","Actual: There are pictures of males and females and their business is all out.\n","Predicted: There are pictures of people and they business is all out.\n","*******************************\n","Actual: there are bad pictures of girls and they business is all out.\n","Predicted: There are pictures of people and they business is all out.\n","*******************************\n","Actual: Difference between a bonafide accident and one from negligence\n","Predicted: difference between a bonafide accident versus one borne from negligence and ignorance.\n","*******************************\n","Actual: your comment is not very smart\n","Predicted: Your comment is not good.\n","*******************************\n","Actual: Your comment is absolutely baseless.\n","Predicted: Your comment is not good.\n","*******************************\n","Actual: I like that black thing\n","Predicted: I like that black thing.\n","*******************************\n","Actual: I like that black.\n","Predicted: I like that black thing.\n","*******************************\n","Actual: How do those actors say those lines without falling to the floor laughing?\n","Predicted: how do those actors say those lines without falling to the floor laughing?\n","*******************************\n","Actual: How do those actors say those lines without failing to the floor laughing?\n","Predicted: how do those actors say those lines without falling to the floor laughing?\n","*******************************\n","Actual: how do those actors say those lines without falling to the floor laughing?\n","Predicted: how do those actors say those lines without falling to the floor laughing?\n","*******************************\n","Actual: Yes. Leave it there and don't let anyone mess you over.\n","Predicted: yeah leave it there girl and don 't let anyone mess you over\n","*******************************\n","Actual: yeah leave it there girl and don 't let anyone take advantage of you\n","Predicted: yeah leave it there girl and don 't let anyone mess you over\n","*******************************\n","Actual: Another thief exited from society\n","Predicted: Another thief exited society.\n","*******************************\n","Actual: How people still think abstinence education work still?\n","Predicted: how does anyone think abstinence education works still?\n","*******************************\n"]}],"source":["for i in range(len(predictions[:15])):\n","    print(f'Actual: {actuals[i]}')\n","    print(f'Predicted: {predictions[i]}')\n","    print('*******************************')"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T13:28:18.149960Z","iopub.status.busy":"2024-02-04T13:28:18.149251Z","iopub.status.idle":"2024-02-04T13:28:54.981338Z","shell.execute_reply":"2024-02-04T13:28:54.980012Z","shell.execute_reply.started":"2024-02-04T13:28:18.149922Z"},"trusted":true},"outputs":[],"source":["!pip install bert_score -q\n","!pip install rouge -q\n","!pip install nltk -q"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T13:35:53.428767Z","iopub.status.busy":"2024-02-04T13:35:53.428356Z","iopub.status.idle":"2024-02-04T13:36:02.721437Z","shell.execute_reply":"2024-02-04T13:36:02.720544Z","shell.execute_reply.started":"2024-02-04T13:35:53.428734Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["calculating scores...\n","computing bert embedding.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3de4d5820f394e51995d4d693cac5300","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/47 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["computing greedy matching.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a456394f13f84830b868849ebe6d6624","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/31 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["done in 5.45 seconds, 363.06 sentences/sec\n","Average BLEU-1 Score: 0.6879232811399469\n","Average BLEU-2 Score: 0.5746359223300971\n","Average BLEU-3 Score: 0.4886637723106609\n","Average BLEU-4 Score: 0.4172496025437202\n","ROUGE Scores: {'rouge-1': {'r': 0.714731259050216, 'p': 0.702241424526046, 'f': 0.702162802289925}, 'rouge-2': {'r': 0.5807024140113957, 'p': 0.573238304625692, 'f': 0.5713386394635447}, 'rouge-l': {'r': 0.7121188969180827, 'p': 0.6999053234608097, 'f': 0.6997358630716541}}\n","BERT Precision: 0.9507128596305847\n","BERT Recall: 0.9539970755577087\n","BERT F1 Score: 0.952174961566925\n"]}],"source":["import nltk\n","from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n","from rouge import Rouge\n","from bert_score import score\n","\n","# BLEU (1-4)\n","bleu_1_scores = [sentence_bleu([ref.split()], pred.split(), weights=(1, 0, 0, 0)) for pred, ref in zip(predictions, actuals)]\n","bleu_2_scores = [sentence_bleu([ref.split()], pred.split(), weights=(0, 1, 0, 0)) for pred, ref in zip(predictions, actuals)]\n","bleu_3_scores = [sentence_bleu([ref.split()], pred.split(), weights=(0, 0, 1, 0)) for pred, ref in zip(predictions, actuals)]\n","bleu_4_scores = [sentence_bleu([ref.split()], pred.split(), weights=(0, 0, 0, 1)) for pred, ref in zip(predictions, actuals)]\n","\n","average_bleu_1 = corpus_bleu([[ref.split()] for ref in actuals], [pred.split() for pred in predictions], weights=(1, 0, 0, 0))\n","average_bleu_2 = corpus_bleu([[ref.split()] for ref in actuals], [pred.split() for pred in predictions], weights=(0, 1, 0, 0))\n","average_bleu_3 = corpus_bleu([[ref.split()] for ref in actuals], [pred.split() for pred in predictions], weights=(0, 0, 1, 0))\n","average_bleu_4 = corpus_bleu([[ref.split()] for ref in actuals], [pred.split() for pred in predictions], weights=(0, 0, 0, 1))\n","\n","# ROUGE\n","rouge = Rouge()\n","rouge_scores = rouge.get_scores(predictions, actuals, avg=True)\n","\n","# BERT-score\n","P, R, F1 = score(predictions, actuals, lang=\"en\", verbose=True)\n","\n","print(\"Average BLEU-1 Score:\", average_bleu_1)\n","print(\"Average BLEU-2 Score:\", average_bleu_2)\n","print(\"Average BLEU-3 Score:\", average_bleu_3)\n","print(\"Average BLEU-4 Score:\", average_bleu_4)\n","print(\"ROUGE Scores:\", rouge_scores)\n","print(\"BERT Precision:\", P.mean().item())\n","print(\"BERT Recall:\", R.mean().item())\n","print(\"BERT F1 Score:\", F1.mean().item())\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
